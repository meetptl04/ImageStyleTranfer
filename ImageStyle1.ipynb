{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObGxF0cioMAYcRCn5fYDcd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetptl04/ImageStyleTranfer/blob/main/ImageStyle1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from tensorflow.keras.preprocessing import image as kp_image\n",
        "\n",
        "# Utility class for image processing\n",
        "class ImageProcessor:\n",
        "    def __init__(self, max_dim=512):\n",
        "        self.max_dim = max_dim\n",
        "\n",
        "    def load_and_process_img(self, path_to_img):\n",
        "        img = cv2.imread(path_to_img)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = self._resize_img(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        img = vgg19.preprocess_input(img)\n",
        "        return img\n",
        "\n",
        "    def _resize_img(self, img):\n",
        "        long = max(img.shape[:2])\n",
        "        scale = self.max_dim / long\n",
        "        new_shape = (int(img.shape[1] * scale), int(img.shape[0] * scale))\n",
        "        img = cv2.resize(img, new_shape)\n",
        "        return img\n",
        "\n",
        "    def deprocess_img(self, processed_img):\n",
        "        x = processed_img.copy()\n",
        "        if len(x.shape) == 4:\n",
        "            x = np.squeeze(x, 0)\n",
        "        x[:, :, 0] += 103.939\n",
        "        x[:, :, 1] += 116.779\n",
        "        x[:, :, 2] += 123.68\n",
        "        x = x[:, :, ::-1]\n",
        "        x = np.clip(x, 0, 255).astype('uint8')\n",
        "        return x\n",
        "\n",
        "# Style Transfer class\n",
        "class StyleTransfer:\n",
        "    def __init__(self, content_img_path, style_img_path):\n",
        "        self.image_processor = ImageProcessor()\n",
        "        self.content_img = self.image_processor.load_and_process_img(content_img_path)\n",
        "        self.style_img = self.image_processor.load_and_process_img(style_img_path)\n",
        "\n",
        "        # Initialize content and style layers\n",
        "        self.content_layers = ['block5_conv2']\n",
        "        self.style_layers = ['block1_conv1',\n",
        "                             'block2_conv1',\n",
        "                             'block3_conv1',\n",
        "                             'block4_conv1',\n",
        "                             'block5_conv1']\n",
        "        self.num_content_layers = len(self.content_layers)\n",
        "        self.num_style_layers = len(self.style_layers)\n",
        "\n",
        "        self.model = self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        vgg = vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "        vgg.trainable = False\n",
        "        outputs = [vgg.get_layer(name).output for name in self.style_layers + self.content_layers]\n",
        "        model = tf.keras.Model([vgg.input], outputs)\n",
        "        return model\n",
        "\n",
        "    def _gram_matrix(self, input_tensor):\n",
        "        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "        input_shape = tf.shape(input_tensor)\n",
        "        num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
        "        return result / num_locations\n",
        "\n",
        "    def _get_content_loss(self, base_content, target):\n",
        "        print(f'Base content shape: {base_content.shape}')\n",
        "        print(f'Target shape: {target.shape}')\n",
        "        return tf.reduce_mean(tf.square(base_content - target))\n",
        "\n",
        "    def _get_style_loss(self, base_style, gram_target):\n",
        "        gram_style = self._gram_matrix(base_style)\n",
        "        print(f'Base style shape: {base_style.shape}')\n",
        "        print(f'Gram target shape: {gram_target.shape}')\n",
        "        return tf.reduce_mean(tf.square(gram_style - gram_target))\n",
        "\n",
        "    def _compute_loss(self, model, loss_weights, init_image, gram_style_features, content_features):\n",
        "        style_weight, content_weight = loss_weights\n",
        "        model_outputs = model(init_image)\n",
        "\n",
        "        # Check shapes\n",
        "        print(f'Model outputs shape: {len(model_outputs)}')\n",
        "        for output in model_outputs:\n",
        "            print(f'Output shape: {output.shape}')\n",
        "\n",
        "        style_output_features = model_outputs[:self.num_style_layers]\n",
        "        content_output_features = model_outputs[self.num_style_layers:]\n",
        "\n",
        "        style_score = 0\n",
        "        content_score = 0\n",
        "\n",
        "        for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
        "            print(f'Gram style feature shape: {target_style.shape}')\n",
        "            print(f'Combined style feature shape: {comb_style.shape}')\n",
        "            style_score += self._get_style_loss(comb_style, target_style)\n",
        "\n",
        "        for target_content, comb_content in zip(content_features, content_output_features):\n",
        "            print(f'Target content shape: {target_content.shape}')\n",
        "            print(f'Combined content feature shape: {comb_content.shape}')\n",
        "            content_score += self._get_content_loss(comb_content, target_content)\n",
        "\n",
        "        style_score *= style_weight / self.num_style_layers\n",
        "        content_score *= content_weight / self.num_content_layers\n",
        "\n",
        "        loss = style_score + content_score\n",
        "        return loss, style_score, content_score\n",
        "\n",
        "    def _compute_grads(self, cfg):\n",
        "        with tf.GradientTape() as tape:\n",
        "            all_loss = self._compute_loss(**cfg)\n",
        "\n",
        "        total_loss = all_loss[0]\n",
        "        return tape.gradient(total_loss, cfg['init_image']), all_loss\n",
        "\n",
        "    def run(self, num_iterations=2000, content_weight=1e3, style_weight=1e-2):\n",
        "        style_features = self.model(self.style_img)[:self.num_style_layers]\n",
        "        content_features = self.model(self.content_img)[self.num_content_layers:]\n",
        "        gram_style_features = [self._gram_matrix(style_feature) for style_feature in style_features]\n",
        "\n",
        "        init_image = tf.Variable(self.content_img, dtype=tf.float32)\n",
        "        opt = tf.optimizers.Adam(learning_rate=5.0)\n",
        "\n",
        "        best_loss, best_img = float('inf'), None\n",
        "\n",
        "        loss_weights = (style_weight, content_weight)\n",
        "        cfg = {\n",
        "            'model': self.model,\n",
        "            'loss_weights': loss_weights,\n",
        "            'init_image': init_image,\n",
        "            'gram_style_features': gram_style_features,\n",
        "            'content_features': content_features\n",
        "        }\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            grads, all_loss = self._compute_grads(cfg)\n",
        "            loss, style_score, content_score = all_loss\n",
        "\n",
        "            # Debugging prints\n",
        "            print(f'Iteration {i}: loss={loss}, style_loss={style_score}, content_loss={content_score}')\n",
        "            print(f'init_image shape: {init_image.shape}')\n",
        "            print(f'grads shape: {grads.shape}')\n",
        "\n",
        "            opt.apply_gradients([(grads, init_image)])\n",
        "            clipped = tf.clip_by_value(init_image, -1.0, 1.0)\n",
        "            init_image.assign(clipped)\n",
        "\n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "                best_img = init_image.numpy()\n",
        "\n",
        "        return self.image_processor.deprocess_img(best_img)\n",
        "\n",
        "# Example usage\n",
        "style_transfer = StyleTransfer('/content/Dancing-House-Praga-1.jpg', '/content/the-starry-night.jpg')\n",
        "result_img = style_transfer.run(num_iterations=500, content_weight=1e3, style_weight=1e-2)\n",
        "\n",
        "# Display the result\n",
        "plt.imshow(result_img)\n",
        "plt.title('Styled Image')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xz7MQOF16uH3",
        "outputId": "422eba03-6fa2-4c02-e1c6-f84cfcf1423e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model outputs shape: 6\n",
            "Output shape: (1, 356, 512, 64)\n",
            "Output shape: (1, 178, 256, 128)\n",
            "Output shape: (1, 89, 128, 256)\n",
            "Output shape: (1, 44, 64, 512)\n",
            "Output shape: (1, 22, 32, 512)\n",
            "Output shape: (1, 22, 32, 512)\n",
            "Gram style feature shape: (1, 64, 64)\n",
            "Combined style feature shape: (1, 356, 512, 64)\n",
            "Base style shape: (1, 356, 512, 64)\n",
            "Gram target shape: (1, 64, 64)\n",
            "Gram style feature shape: (1, 128, 128)\n",
            "Combined style feature shape: (1, 178, 256, 128)\n",
            "Base style shape: (1, 178, 256, 128)\n",
            "Gram target shape: (1, 128, 128)\n",
            "Gram style feature shape: (1, 256, 256)\n",
            "Combined style feature shape: (1, 89, 128, 256)\n",
            "Base style shape: (1, 89, 128, 256)\n",
            "Gram target shape: (1, 256, 256)\n",
            "Gram style feature shape: (1, 512, 512)\n",
            "Combined style feature shape: (1, 44, 64, 512)\n",
            "Base style shape: (1, 44, 64, 512)\n",
            "Gram target shape: (1, 512, 512)\n",
            "Gram style feature shape: (1, 512, 512)\n",
            "Combined style feature shape: (1, 22, 32, 512)\n",
            "Base style shape: (1, 22, 32, 512)\n",
            "Gram target shape: (1, 512, 512)\n",
            "Target content shape: (1, 178, 256, 128)\n",
            "Combined content feature shape: (1, 22, 32, 512)\n",
            "Base content shape: (1, 22, 32, 512)\n",
            "Target shape: (1, 178, 256, 128)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:Sub] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f95261dbcabb>\u001b[0m in \u001b[0;36m<cell line: 160>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0mstyle_transfer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyleTransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Dancing-House-Praga-1.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/the-starry-night.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0mresult_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_transfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;31m# Display the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-f95261dbcabb>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_iterations, content_weight, style_weight)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-f95261dbcabb>\u001b[0m in \u001b[0;36m_compute_grads\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mall_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-f95261dbcabb>\u001b[0m in \u001b[0;36m_compute_loss\u001b[0;34m(self, model, loss_weights, init_image, gram_style_features, content_features)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Target content shape: {target_content.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Combined content feature shape: {comb_content.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mcontent_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_content_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mstyle_score\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mstyle_weight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_style_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-f95261dbcabb>\u001b[0m in \u001b[0;36m_get_content_loss\u001b[0;34m(self, base_content, target)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Base content shape: {base_content.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Target shape: {target.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_content\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_style_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_style\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgram_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5982\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5983\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:Sub] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NAl8aJel7UF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}